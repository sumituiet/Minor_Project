{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCM7B5443rJU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as ny\n",
        "import whois\n",
        "import json\n",
        "import requests\n",
        "import datetime\n",
        "import urllib.request\n",
        "import regex\n",
        "from tldextract import extract\n",
        "import ssl\n",
        "import socket\n",
        "import favicon\n",
        "import sys, re\n",
        "import xmltodict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhOS4ewn3rJa"
      },
      "outputs": [],
      "source": [
        "def url_having_ip(url):\n",
        "#using regular function\n",
        "    symbol = regex.findall(r'(http((s)?)://)((((\\d)+).)*)((\\w)+)(/((\\w)+))?',url)\n",
        "    if(len(symbol)!=0):\n",
        "        having_ip = 1 #phishing\n",
        "\n",
        "    else:\n",
        "        having_ip = -1 #legitimate\n",
        "    return(having_ip)\n",
        "    return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_9x6Vvk3rJd"
      },
      "outputs": [],
      "source": [
        "def url_length(url):\n",
        "    length=len(url)\n",
        "    if(length<54):\n",
        "        return -1\n",
        "    elif(54<=length<=75):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PghTxRkk3rJi"
      },
      "outputs": [],
      "source": [
        "def url_short(url):\n",
        "    try:\n",
        "        r = requests.get('http://techtv.mit.edu/videos/1585-music-session-02/download.source')\n",
        "        for i in r.history:\n",
        "            print(i.url)\n",
        "            return 0\n",
        "    except Exception as e:\n",
        "            return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcWniZUi3rJq"
      },
      "outputs": [],
      "source": [
        "def having_at_symbol(url):\n",
        "    symbol=regex.findall(r'@',url)\n",
        "    if(len(symbol)==0):\n",
        "        return -1\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT9FckBG3rJw"
      },
      "outputs": [],
      "source": [
        "def prefix_suffix(url):\n",
        "    subDomain, domain, suffix = extract(url)\n",
        "    if(domain.count('-')):\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAT2Fbdq3rJz"
      },
      "outputs": [],
      "source": [
        "def sub_domain(url):\n",
        "    subDomain, domain, suffix = extract(url)\n",
        "    if(subDomain.count('.')==0):\n",
        "        return -1\n",
        "    elif(subDomain.count('.')==1):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYNlz1Yu3rJ2"
      },
      "outputs": [],
      "source": [
        "def SSLfinal_State(url):\n",
        "    try:\n",
        "#check wheather contains https\n",
        "        if(regex.search('^https',url)):\n",
        "            usehttps = 1\n",
        "        else:\n",
        "            usehttps = 0\n",
        "#getting the certificate issuer to later compare with trusted issuer\n",
        "        #getting host name\n",
        "        subDomain, domain, suffix = extract(url)\n",
        "        host_name = domain + \".\" + suffix\n",
        "        context = ssl.create_default_context()\n",
        "        sct = context.wrap_socket(socket.socket(), server_hostname = host_name)\n",
        "        sct.connect((host_name, 443))\n",
        "        certificate = sct.getpeercert()\n",
        "        issuer = dict(x[0] for x in certificate['issuer'])\n",
        "        certificate_Auth = str(issuer['commonName'])\n",
        "        certificate_Auth = certificate_Auth.split()\n",
        "        if(certificate_Auth[0] == \"Network\" or certificate_Auth == \"Deutsche\"):\n",
        "            certificate_Auth = certificate_Auth[0] + \" \" + certificate_Auth[1]\n",
        "        else:\n",
        "            certificate_Auth = certificate_Auth[0]\n",
        "        trusted_Auth = ['Comodo','Symantec','GoDaddy','GlobalSign','DigiCert','StartCom','Entrust','Verizon','Trustwave','Unizeto','Buypass','QuoVadis','Deutsche Telekom','Network Solutions','SwissSign','IdenTrust','Secom','TWCA','GeoTrust','Thawte','Doster','VeriSign']\n",
        "#getting age of certificate\n",
        "        startingDate = str(certificate['notBefore'])\n",
        "        endingDate = str(certificate['notAfter'])\n",
        "        startingYear = int(startingDate.split()[3])\n",
        "        endingYear = int(endingDate.split()[3])\n",
        "        Age_of_certificate = endingYear-startingYear\n",
        "\n",
        "#checking final conditions\n",
        "        if((usehttps==1) and (certificate_Auth in trusted_Auth) and (Age_of_certificate>=1) ):\n",
        "            return -1 #legitimate\n",
        "        elif((usehttps==1) and (certificate_Auth not in trusted_Auth)):\n",
        "            return 0 #suspicious\n",
        "        else:\n",
        "            return 1 #phishing\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWgVXUvj3rJ5"
      },
      "outputs": [],
      "source": [
        "def domain_registration(url):\n",
        "    try:\n",
        "        w = whois.whois(url)\n",
        "        updated = w.updated_date\n",
        "        exp = w.expiration_date\n",
        "        length = (exp[0]-updated[0]).days\n",
        "        if(length<=365):\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeehUvKW3rJ8"
      },
      "outputs": [],
      "source": [
        "def https_token(url):\n",
        "    subDomain, domain, suffix = extract(url)\n",
        "    host =subDomain +'.' + domain + '.' + suffix\n",
        "    if(host.count('https')): #attacker can trick by putting https in domain part\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKhi7uZ53rJ_"
      },
      "outputs": [],
      "source": [
        "def request_url(url):\n",
        "    try:\n",
        "        subDomain, domain, suffix = extract(url)\n",
        "        websiteDomain = domain\n",
        "\n",
        "        opener = urllib.request.urlopen(url).read()\n",
        "        soup = BeautifulSoup(opener, 'lxml')\n",
        "        imgs = soup.findAll('img', src=True)\n",
        "        total = len(imgs)\n",
        "\n",
        "        linked_to_same = 0\n",
        "        avg =0\n",
        "        for image in imgs:\n",
        "            subDomain, domain, suffix = extract(image['src'])\n",
        "            imageDomain = domain\n",
        "            if(websiteDomain==imageDomain or imageDomain==''):\n",
        "                linked_to_same = linked_to_same + 1\n",
        "        vids = soup.findAll('video', src=True)\n",
        "        total = total + len(vids)\n",
        "\n",
        "        for video in vids:\n",
        "            subDomain, domain, suffix = extract(video['src'])\n",
        "            vidDomain = domain\n",
        "            if(websiteDomain==vidDomain or vidDomain==''):\n",
        "                linked_to_same = linked_to_same + 1\n",
        "        linked_outside = total-linked_to_same\n",
        "        if(total!=0):\n",
        "            avg = linked_outside/total\n",
        "\n",
        "        if(avg<0.22):\n",
        "            return -1\n",
        "        elif(0.22<=avg<=0.61):\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c7dg_9o3rKD"
      },
      "outputs": [],
      "source": [
        "def url_of_anchor(url):\n",
        "    try:\n",
        "        subDomain, domain, suffix = extract(url)\n",
        "        websiteDomain = domain\n",
        "\n",
        "        opener = urllib.request.urlopen(url).read()\n",
        "        soup = BeautifulSoup(opener, 'lxml')\n",
        "        anchors = soup.findAll('a', href=True)\n",
        "        total = len(anchors)\n",
        "        linked_to_same = 0\n",
        "        avg = 0\n",
        "        for anchor in anchors:\n",
        "            subDomain, domain, suffix = extract(anchor['href'])\n",
        "            anchorDomain = domain\n",
        "            if(websiteDomain==anchorDomain or anchorDomain==''):\n",
        "                linked_to_same = linked_to_same + 1\n",
        "        linked_outside = total-linked_to_same\n",
        "        if(total!=0):\n",
        "            avg = linked_outside/total\n",
        "\n",
        "        if(avg<0.31):\n",
        "            return -1\n",
        "        elif(0.31<=avg<=0.67):\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtA5zk5E3rKG"
      },
      "outputs": [],
      "source": [
        "def Links_in_tags(url):\n",
        "    try:\n",
        "        opener = urllib.request.urlopen(url).read()\n",
        "        soup = BeautifulSoup(opener, 'lxml')\n",
        "\n",
        "        no_of_meta =0\n",
        "        no_of_link =0\n",
        "        no_of_script =0\n",
        "        anchors=0\n",
        "        avg =0\n",
        "        for meta in soup.find_all('meta'):\n",
        "            no_of_meta = no_of_meta+1\n",
        "        for link in soup.find_all('link'):\n",
        "            no_of_link = no_of_link +1\n",
        "        for script in soup.find_all('script'):\n",
        "            no_of_script = no_of_script+1\n",
        "        for anchor in soup.find_all('a'):\n",
        "            anchors = anchors+1\n",
        "        total = no_of_meta + no_of_link + no_of_script+anchors\n",
        "        tags = no_of_meta + no_of_link + no_of_script\n",
        "        if(total!=0):\n",
        "            avg = tags/total\n",
        "\n",
        "        if(avg<0.25):\n",
        "            return -1\n",
        "        elif(0.25<=avg<=0.81):\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L5fpEdz3rKJ"
      },
      "outputs": [],
      "source": [
        "def email_submit(url):\n",
        "    try:\n",
        "        opener = urllib.request.urlopen(url).read()\n",
        "        soup = BeautifulSoup(opener, 'lxml')\n",
        "        if(soup.find('mailto:')):\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTZNLgia3rKM"
      },
      "outputs": [],
      "source": [
        "def age_of_domain(url):\n",
        "    try:\n",
        "        w = whois.whois(url)\n",
        "        start_date = w.creation_date\n",
        "        current_date = datetime.datetime.now()\n",
        "        age =(current_date-start_date[0]).days\n",
        "        if(age>=180):\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return 0\n",
        "def statistical(url):\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYbRZt7Q3rKP"
      },
      "outputs": [],
      "source": [
        "def fav(url):\n",
        "    try:\n",
        "        icons = favicon.get(url)\n",
        "        if len(icons)>0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    except Exception as e:\n",
        "        return 1\n",
        "\n",
        "def sfh(url):\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S0Grrvv3rKa"
      },
      "outputs": [],
      "source": [
        "def web_traffic(url):\n",
        "        try:\n",
        "            xml = urllib.request.urlopen('http://data.alexa.com/data?cli=10&dat=s&url={}'.format(url)).read()\n",
        "            result= xmltodict.parse(xml)\n",
        "            data = json.dumps(result).replace(\"@\",\"\")\n",
        "            data_tojson = json.loads(data)\n",
        "\n",
        "            url = data_tojson[\"ALEXA\"][\"SD\"][1][\"POPULARITY\"][\"URL\"]\n",
        "            rank= data_tojson[\"ALEXA\"][\"SD\"][1][\"POPULARITY\"][\"TEXT\"]\n",
        "            Ranki = int(rank)\n",
        "            if Ranki>=10000:\n",
        "                return 1\n",
        "            elif Ranki<=10000:\n",
        "                return -1\n",
        "            else:\n",
        "                return 0\n",
        "        except KeyError:\n",
        "            return 1\n",
        "def abnormal_url(url):\n",
        "    return 0\n",
        "def redirect(url):\n",
        "    return 0\n",
        "def links_pointing(url):\n",
        "    return 0\n",
        "def page_rank(url):\n",
        "    try:\n",
        "        xml = urllib.request.urlopen('http://data.alexa.com/data?cli=10&dat=s&url={}'.format(url)).read()\n",
        "        result= xmltodict.parse(xml)\n",
        "        data = json.dumps(result).replace(\"@\",\"\")\n",
        "        data_tojson = json.loads(data)\n",
        "\n",
        "        url = data_tojson[\"ALEXA\"][\"SD\"][1][\"POPULARITY\"][\"URL\"]\n",
        "        rank= data_tojson[\"ALEXA\"][\"SD\"][1][\"POPULARITY\"][\"TEXT\"]\n",
        "        Ranki = int(rank)\n",
        "        if Ranki>=10000:\n",
        "            return 1\n",
        "        elif Ranki<=10000:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "    except KeyError:\n",
        "        return 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5qZWliP3rKg"
      },
      "outputs": [],
      "source": [
        "def main(url):\n",
        "    check = [[url_having_ip(url),url_length(url),url_short(url),having_at_symbol(url),\n",
        "              prefix_suffix(url),sub_domain(url),SSLfinal_State(url),\n",
        "              domain_registration(url),fav(url),https_token(url),request_url(url),\n",
        "              url_of_anchor(url),Links_in_tags(url),sfh(url),email_submit(url),abnormal_url(url),\n",
        "              redirect(url),age_of_domain(url),web_traffic(url),page_rank(url),\n",
        "              links_pointing(url),statistical(url)]]\n",
        "    print(check)\n",
        "    return check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "-GwVrnF83rKo",
        "outputId": "938ed24a-0e28-4505-d79b-c40c00ad956c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.google.com/\n",
            "http://techtv.mit.edu/videos/1585-music-session-02/download.source\n",
            "timed out\n",
            "[[1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0]]\n",
            "[[1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "url = input()\n",
        "checko = main(url)\n",
        "print(checko)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-9iR_iHmb_u"
      },
      "outputs": [],
      "source": [
        "#type(page_rank.ranki)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4hTxCTj73rKv",
        "outputId": "b1a98201-4c56-493a-b620-3bf8423e7a86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(checko)\n",
        "test = ny.asarray(checko)\n",
        "type(test)\n",
        "#len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "ooW-gLJI3rKz",
        "outputId": "a4d24d45-5841-4c78-e2a5-c59e2064ddfa",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.112]\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import pickle\n",
        "\n",
        "#with open(\"model.pkl\",\"rb\") as f:\n",
        "#    model = pickle.load(f)=[]\n",
        "#with open('model.plk', 'rb') as infile:\n",
        "#    model = pickle.load(infile)\n",
        "#print(type(classifier))\n",
        "classifier = joblib.load(\"Model.sav\")\n",
        "#checkprediction = main(url)\n",
        "predictions = classifier.predict(checko)\n",
        "\n",
        "# prediction = classifier.main(url)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "D6KzDc4u3rK6",
        "outputId": "2173e642-9586-4450-89fc-3139994012de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sklearn.ensemble.forest.RandomForestRegressor"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwpRzfxm6gGr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}